[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "Hi!\nI‚Äôm Francisco and this is my personal blog/website, where I‚Äôll be sharing technical tutorials, thoughts, and whatever I see fit!\nFeel free to look around!"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Tutorial: Reproducible Spark+Delta Tests Without the Hassle\n\n\n\nspark\n\ndelta\n\ndocker\n\n\n\n\n\n\n\n\n\nOct 23, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/2025-tutorial-spark-connect/index.html",
    "href": "blog/2025-tutorial-spark-connect/index.html",
    "title": "Tutorial: Reproducible Spark+Delta Tests Without the Hassle",
    "section": "",
    "text": "Pre-requisite: if you‚Äôre not familiar with Docker, I recommend you follow the official introductory guide.\nSetting up a Spark instance can be quite the hassle. Add a data catalog like Delta or Apache Iceberg, and the complexity blows up!\nBut testing catalog-specific queries against a Spark instance should be easy. And, if you‚Äôre an automation nerd like myself, it should be reproducible, as well.\nUnfortunately, the traditional method of installing the OpenJDK, JVM, Spark drivers, Hive plugins, Delta jars is anything but easy, let alone reproducible (As a matter of fact, on the eve of publishing this article, the Live Notebooks on PySpark‚Äôs official quickstart page failed to launch due to a bug downloading OpenJDK‚Ä¶ üòÖ).\nFaced with this problem, I decided to take matters into my own hands ‚Äî and published a Docker Hub repository for running Spark Connect servers!"
  },
  {
    "objectID": "blog/2025-tutorial-spark-connect/index.html#running-the-container",
    "href": "blog/2025-tutorial-spark-connect/index.html#running-the-container",
    "title": "Tutorial: Reproducible Spark+Delta Tests Without the Hassle",
    "section": "Running the Container",
    "text": "Running the Container\nRunning it is as straight-forward as any other container. Simply:\ndocker run -p 15002:15002 franciscoabsampaio/spark-connect-server:delta\nWe use the ‚Äúdelta‚Äù tag if we wish to spin the server with a Delta catalog, and the ‚Äúiceberg‚Äù tag for the Iceberg catalog.\nThe docker run command will pull the image, and start it, resulting in a generous slew of logs, ending with a message akin to ‚ÄúSpark Connect server started at 0.0.0.0:15002‚Äù ‚Äî letting us know that the Spark instance is LIVE and ready to accept connections.\nNow, connecting to it in PySpark is as easy as:\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .remote(\"sc://localhost:15002\") \\\n    .getOrCreate()\n\n# A simple check:\ndf = spark.sql(f\"SELECT 1 AS id\")\nrows = df.collect()\n\nprint(rows)\n\nNote: You may have to install Python packages grpcio, grpcio-status and protobuf for pyspark to connect successfully.\n\nThe Spark session is ON and completely functional for all you development needs."
  },
  {
    "objectID": "blog/2025-tutorial-spark-connect/index.html#spark-connect",
    "href": "blog/2025-tutorial-spark-connect/index.html#spark-connect",
    "title": "Tutorial: Reproducible Spark+Delta Tests Without the Hassle",
    "section": "Spark Connect",
    "text": "Spark Connect\nSpark Connect is Apache‚Äôs newest gRPC protocol for Apache Spark. It lets any client that implements the Spark Connect protocol to connect to a Spark Server, and execute workloads seamlessly.\nWhat this Docker container does is take care of all the setup for launching a Spark instance with Spark Connect server, to make the development experience as smooth as possible."
  },
  {
    "objectID": "blog/2025-tutorial-spark-connect/index.html#reproducible-tests",
    "href": "blog/2025-tutorial-spark-connect/index.html#reproducible-tests",
    "title": "Tutorial: Reproducible Spark+Delta Tests Without the Hassle",
    "section": "Reproducible Tests",
    "text": "Reproducible Tests\nIf you‚Äôd like to take things a step further, we can manage the Spark server‚Äôs lifecycle entirely in our test suite, ensuring reproducible, specified-in-code, end-to-end dependencies.\nTo do so, we can create a pytest fixture, which is essentially a reusable resource for tests!\nimport docker\n# You can use testcontainers instead of the docker package\n# but I find it a bit bloated and buggy\n# when working with custom images.\n# The docker package is what testcontainers uses\n# behind the scenes, and I find it more consistent.\nimport pytest\nimport time\n\n\n# Utility function:\n# Once the container has produced the desired message, return.\ndef wait_for_log(container, message, timeout=30):\n    start = time.time()\n    while True:\n        logs = container.logs().decode(\"utf-8\")\n        if message in logs:\n            return\n        if time.time() - start &gt; timeout:\n            raise TimeoutError(f\"Message '{message}' not found in logs\")\n        time.sleep(0.5)\n\n\n# The default scope is function,\n# which means the fixture is created and destroyed every test.\n@pytest.fixture(scope=\"function\")\ndef spark_connect_server_url():\n    docker_client = docker.from_env()\n\n    # Let's run a Spark Connect server with a Delta catalog!\n    container = docker_client.containers.run(\n        image=\"franciscoabsampaio/spark-connect-server:delta\",\n        detach=True,\n        ports={'15002/tcp': 15002}\n    )\n    wait_for_log(container, message=\"Spark Connect server started\")\n\n    # Pass the connection string to whoever is using the fixture.\n    yield \"sc://localhost:15002\"\n\n    # Cleanup\n    container.stop()\nNow, we can simply pass the fixture to whichever test requires a running Spark Connect server. For example:\ndef test_basic_read(spark_connect_server_url):  # Use the fixture\n    spark = SparkSession.builder \\\n        .remote(spark_connect_server_url) \\\n        .getOrCreate()\n\n    table_name = \"test_table\"\n\n    # CREATE TABLE USING DELTA\n    spark.sql(f\"\"\"\n        CREATE TABLE {table_name} (\n            id INT,\n            name STRING\n        )\n        USING DELTA\n    \"\"\")\n\n    # INSERT INTO TABLE\n    spark.sql(f\"INSERT INTO {table_name} VALUES (1, 'bird'), (2, 'spark')\")\n\n    # SELECT * FROM TABLE\n    df = spark.sql(f\"SELECT * FROM {table_name} ORDER BY id\")\n    rows = df.collect()\n\n    # Test conditions\n    assert len(rows) == 2\n    assert rows[0][\"id\"] == 1\n    assert rows[0][\"name\"] == \"bird\"\nThe yield statement locks the fixture execution until the test finishes ‚Äî at which point the cleanup code is executed.\n\nAnd that‚Äôs it ‚Äî you now have a fully reproducible, containerized Spark + Delta test setup, ready to integrate into any CI/CD pipeline or local test suite.\nYou can find the container on Docker Hub: üëâ franciscoabsampaio/spark-connect-server\nIf this saved you time (or frustration), give it a ‚≠êÔ∏è on GitHub or share it with your team ‚Äî and let me know what feature you‚Äôd like supported next!\nHappy testing! üöÄ"
  },
  {
    "objectID": "blog/2025-2b-model-at-home/index.html",
    "href": "blog/2025-2b-model-at-home/index.html",
    "title": "Exploring Data with Python",
    "section": "",
    "text": "In this post we‚Äôll do a quick exploration using Python and pandas.\n# Create sample data\nprint(123)\n\n123"
  },
  {
    "objectID": "blog/2025-2b-model-at-home/index.html#activation-checkpoints",
    "href": "blog/2025-2b-model-at-home/index.html#activation-checkpoints",
    "title": "Exploring Data with Python",
    "section": "Activation Checkpoints",
    "text": "Activation Checkpoints\nActivation checkpointing consists of selectively forgetting activations during each training loop, effectively trading off compute for a shallower memory footprint. The concept (expertly presented in this blog post by Bulatov (2018)) is a clever exploitation of dependencies in the model graph that can reduce memory requirements during training from O(n^2) to O(sqrt(n)) - all for the modest cost of an extra forward pass.\nIf we picture the training loop as a directed graph, with each layer‚Äôs activations as nodes on the graph, and activations triggering dependent nodes in a cascading fashion, one way of managing our GPU‚Äôs memory would be to store every node‚Äôs values until a complete pass of the training loop is complete. Clearly, this would not be a good idea if - after all, why store the activations of the first nodes in the graph if we only need the preceding node‚Äôs activations for any given node? It would be much more memory efficient to forget node values as we progress through the graph:\n\n\n\nRich man‚Äôs activation checkpointing (Bulatov 2018).\n\n\nA big problem with training is that the backward pass starts from the loss node (computed from the last node of the forward pass) and moves in reverse order - if we only vacate computed nodes from memory after they‚Äôre no longer needed, we will be keeping a lot of forward pass activations as we walk through the backward pass. The poor man‚Äôs solution would be to forget every node as soon as it is consumed - but this results in a quadratic increase of computations:\n\n\n\nPoor man‚Äôs activation checkpointing (Bulatov 2018).\n\n\nChen et al. (2016) found an effective middle ground: by saving intermediate nodes every sqrt(n) steps - checkpoints - and forgetting all those that won‚Äôt be needed in the next step, the memory footprint can be greatly reduced, for the modest cost of a second forward pass‚Äô worth of compute:\n\n\n\nSmart man‚Äôs activation checkpointing (Bulatov 2018).\n\n\nThis pebble game we‚Äôre playing can get extremely complex (dynamic programming, tree decomposition, etc), but this much will suffice us."
  }
]