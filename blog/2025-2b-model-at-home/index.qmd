---
title: "Exploring Data with Python"
description: "A quick example of embedding Python code and plots in a Quarto post."
date: 2025-11-20
categories: [python, data]
bibliography: references.bib
execute:
  echo: true
  warning: false
  message: false
jupyter: python3
draft: true
---

In this post weâ€™ll do a quick exploration using Python and pandas.

```{python}
# Create sample data
print(123)
```

## Mixed Precision

## Online Softmax and FlashAttention

- motivation for why we would like to make attention more performant
- explain the main issue with attention is slow memory access

Most operations in Transformers are bottlenecked by memory access [@2022_flashattention].

GPUs have two main kinds of memory - on-chip SRAM (shared memory) and HBM (global memory)
As an example, NVIDIA's V100 GPU has 40-80 GB of high-bandwidth memory (HBM) with a bandwidth of 1.5-2.0 TB/s and 192KB of on-chip shared memory (SRAM) for every 108 streaming multiprocessors with bandwidth estimated around 19 TB/s [@2022_flashattention] - about an order of magnitude faster.

FlashAttention (FA) [@2022_flashattention] was introduced in an effort to minimize access to HBM.

One amazing fact about FlashAttention is that we don't need to materialize X and A matrices
on global memory, instead we fuse the entire computation in formula 1 in a single CUDA kernel.

A common approach to accelerate memory-bound operations is termed **kernel fusion**, that is, the fusion of multiple operations (kernels) into a single set of instructions, such that only a single access to memory is needed.

Hence, this requires:
- Computing the softmax reduction without access to the whole input
- Not storing any of the large intermediate matrices, namely, for backward passes

us to design an algorithm that carefully manages on-chip memory because NVIDIA GPU's shared memory (we want to allocate to SRAM because HBM is much slower)

The softmax algorithm is ???[explain softmax]. Because exponents can easily overflow (e.g. if the maximum number that float16 can support is 65536, then $x\geq11$ overflows [@2023_from_online_softmax_to_flashattention]), algorithms usually subtract the maximum $x_i$ in the sequence from the exponent (multiplying both the numerator and denominator by $e^{-x_{max}}$ - the same as multiplying by $\frac{1}{1}$) to prevent this. This is known as the safe softmax. Unfortunately, determining the maximum of a sequence requires an additional memory access across *all blocks of the $QK^T$ matrix*, which not only hurts performance, but any hope of efficiently parallelizing the softmax operation.

Online Softmax [@2018_online_softmax] was an improvement on the original softmax algorithm, which allows the softmax to be computed in a running fashion, unlocking **performance gains**, **distributed runs**, and **longer contexts**. It uses a neat little trick we won't be delving into here. If you want to learn more, @2024_flashattention_umar explains it very well, tying it elegantly to FlashAttention.

Although Online Softmax lets us reduce the number of global memory accesses from 3 to 2, there is no way of trimming it to just 1 global I/O.

*Surprisingly*, when the remaining attention formula is thrown into the mix, it becomes possible. **This is the breakthrough of FlashAttention** [@2022_flashattention]. The algorithm is compatible with matrix tiling and fits into a GPU's shared memory.

[explain FA]

the authors confirm that FA speeds up model training and improves model quality by modelling longer context.

[perhaps explain FA2 here? and change the paragpraph that comes after]

Since the original FA algorithm have been proposed, upgrades to the algorithm have been proposed, namely FA2 [@2023_flashattention_2] and FA3 [@2024_flashattention_3], as well as FA4 (work-in-progress) [@2024_flashattention_4]. It is important to understand that whereas FA and FA2 are the result of the original attention algorithm's lack of IO awareness, FA3 and FA4 go above and beyond hardware-awareness - they are *hardware-specific*. FA3 and FA4 were proposed to deliberately take advantage of the unique features of NVIDIA's H100 and B200 chips, respectively. To do so, they incorporate low-level hardware concepts into the algorithms themselves, in order to achieve the highest possible level of GPU utilization.

The practical consequence of this is that for those of us not running datacenter-grade GPUs, FA3 and FA4 will not yield any significant improvement, and may instead hinder performance, if they can even run.

## Activation Checkpoints

Activation checkpointing consists of selectively forgetting activations during each training loop, effectively trading off compute for a shallower memory footprint. The concept (expertly presented in [this blog post](https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9) by @2018_medium_activation_checkpoint) is a clever exploitation of dependencies in the model graph that can reduce memory requirements during training from `O(n^2)` to `O(sqrt(n))` - all for the modest cost of an extra forward pass.

If we picture the training loop as a directed graph, with each layer's activations as nodes on the graph, and activations triggering dependent nodes in a cascading fashion, one way of managing our GPU's memory would be to store every node's values until a complete pass of the training loop is complete.
Clearly, this would not be a good idea if - after all, why store the activations of the first nodes in the graph if we only need the preceding node's activations for any given node? It would be much more memory efficient to forget node values as we progress through the graph:

![Rich man's activation checkpointing [@2018_medium_activation_checkpoint].](images/activation_checkpoint_rich.webp "Rich man's activation checkpointing")

A big problem with training is that the backward pass starts from the loss node (computed from the last node of the forward pass) and moves in reverse order - if we only vacate computed nodes from memory after they're no longer needed, we will be keeping a lot of forward pass activations as we walk through the backward pass. The poor man's solution would be to forget every node *as soon as it is consumed* - but this results in a quadratic increase of computations:

![Poor man's activation checkpointing [@2018_medium_activation_checkpoint].](images/activation_checkpoint_poor.webp "Poor man's activation checkpointing")

@2016_activation_checkpoint found an effective middle ground: by saving intermediate nodes every `sqrt(n)` steps - **checkpoints** - and forgetting all those that won't be needed in the next step, the memory footprint can be greatly reduced, for the modest cost of a second forward pass' worth of compute:

![Smart man's activation checkpointing [@2018_medium_activation_checkpoint].](images/activation_checkpoint_smart.webp "Smart man's activation checkpointing")

[This pebble game we're playing](https://en.wikipedia.org/wiki/Pebble_game) can get extremely complex (dynamic programming, tree decomposition, etc), but this much will suffice us.

## Data Parallelism, Model Parallelism and Pipelining

1. Data Parallelism
2. Model Parallelism
3. Pipelining
