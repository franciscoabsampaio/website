@article{2016_activation_checkpoint,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}

@online{2018_medium_activation_checkpoint,
  author    = {Yaroslav Bulatov},
  title     = {Fitting larger networks into memory.},
  year      = {2018},
  url       = {https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9},
  note      = {Medium, accessed 20 November 2025}
}

@article{2018_online_softmax,
  title={Online normalizer calculation for softmax},
  author={Milakov, Maxim and Gimelshein, Natalia},
  journal={arXiv preprint arXiv:1805.02867},
  year={2018}
}

@article{2022_flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@online{2023_from_online_softmax_to_flashattention,
  author    = {Zihao Ye},
  title     = {From Online Softmax to FlashAttention},
  year      = {2023},
  url       = {https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf},
  note      = {University of Washington, accessed 24 November 2025}
}

@online{2024_flashattention_umar,
  author    = {Umar Jamil},
  title     = {Flash Attention derived and coded from first principles with Triton (Python)},
  year      = {2024},
  url       = {https://www.youtube.com/watch?v=zy8ChVd_oTM},
  note      = {YouTube, accessed 22 November 2025}
}