@article{2016_activation_checkpoint,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}

@online{2018_medium_activation_checkpoint,
  author    = {Yaroslav Bulatov},
  title     = {Fitting larger networks into memory.},
  year      = {2018},
  url       = {https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9},
  note      = {Medium, accessed 20 November 2025}
}

@article{2018_online_softmax,
  title={Online normalizer calculation for softmax},
  author={Milakov, Maxim and Gimelshein, Natalia},
  journal={arXiv preprint arXiv:1805.02867},
  year={2018}
}

@article{2022_flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@online{2023_from_online_softmax_to_flashattention,
  author    = {Zihao Ye},
  title     = {From Online Softmax to FlashAttention},
  year      = {2023},
  url       = {https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf},
  note      = {University of Washington, accessed 24 November 2025}
}

@article{2023_flashattention_2,
  title={Flashattention-2: Faster attention with better parallelism and work partitioning},
  author={Dao, Tri},
  journal={arXiv preprint arXiv:2307.08691},
  year={2023}
}

@online{2024_flashattention_umar,
  author    = {Umar Jamil},
  title     = {Flash Attention derived and coded from first principles with Triton (Python)},
  year      = {2024},
  url       = {https://www.youtube.com/watch?v=zy8ChVd_oTM},
  note      = {YouTube, accessed 22 November 2025}
}

@article{2024_flashattention_3,
  title={Flashattention-3: Fast and accurate attention with asynchrony and low-precision},
  author={Shah, Jay and Bikshandi, Ganesh and Zhang, Ying and Thakkar, Vijay and Ramani, Pradeep and Dao, Tri},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={68658--68685},
  year={2024}
}

@online{2024_flashattention_4,
  author    = {Frye, Charles and Wang, Nathan and Feng, Timothy},
  title     = {We reverse-engineered Flash Attention 4},
  year      = {2024},
  url       = {https://modal.com/blog/reverse-engineer-flash-attention-4},
  note      = {Modal.com, accessed 27 November 2025}
}
