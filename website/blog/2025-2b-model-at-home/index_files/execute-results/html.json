{
  "hash": "96537cb2652307848f2e372f15e6fb66",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Exploring Data with Python\"\ndescription: \"A quick example of embedding Python code and plots in a Quarto post.\"\ndate: 2025-11-20\ncategories: [python, data]\nbibliography: references.bib\nexecute:\n  echo: true\n  warning: false\n  message: false\njupyter: python3\n---\n\nIn this post weâ€™ll do a quick exploration using Python and pandas.\n\n::: {#fd74a536 .cell execution_count=1}\n``` {.python .cell-code}\n# Create sample data\nprint(123)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n123\n```\n:::\n:::\n\n\n## Activation Checkpoints\n\nActivation checkpointing consists of selectively forgetting activations during each training loop, effectively trading off compute for a shallower memory footprint. The concept (expertly presented in [this blog post](https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9) by @2018_medium_activation_checkpoint) is a clever exploitation of dependencies in the model graph that can reduce memory requirements during training from `O(n^2)` to `O(sqrt(n))` - all for the modest cost of an extra forward pass.\n\nIf we picture the training loop as a graph, with each layer's activations as nodes on the graph, and activations triggering dependent nodes in a cascading fashion, one way of managing our GPU's memory would be to store every node's values until a complete pass of the training loop is complete. \nClearly, this would not be a good idea if - after all, why store the activations of the first nodes in the graph if we only need the preceding node's activations for any given node? It would be much more memory efficient to forget node values as we progress through the graph:\n\n![Rich man's activation checkpointing [@2018_medium_activation_checkpoint].](images/activation_checkpoint_rich.webp \"Rich man's activation checkpointing\")\n\nA big problem with training is that the backward pass starts from the loss node (computed from the last node of the forward pass) and moves in reverse order - if we only vacate computed nodes from memory after they're no longer needed, we will be keeping a lot of forward pass activations as we walk through the backward pass. The poor man's solution would be to forget every node *as soon as it is consumed* - but this results in a quadratic increase of computations:\n\n![Poor man's activation checkpointing [@2018_medium_activation_checkpoint].](images/activation_checkpoint_poor.webp \"Poor man's activation checkpointing\")\n\n@2016_activation_checkpoint found an effective middle ground: by saving intermediate nodes every `sqrt(n)` steps - **checkpoints** - and forgetting all those that won't be needed in the next step, the memory footprint can be greatly reduced, for the modest cost of a second forward pass' worth of compute:\n\n![Smart man's activation checkpointing [@2018_medium_activation_checkpoint].](images/activation_checkpoint_smart.webp \"Smart man's activation checkpointing\")\n\nAs graphs and requirements get more complex, so does the checkpointing strategy, but\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}